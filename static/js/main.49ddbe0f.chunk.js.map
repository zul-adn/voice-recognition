{"version":3,"sources":["logo.svg","App.js","reportWebVitals.js","index.js"],"names":["App","React","useState","isTalk","setIsTalk","words","setWords","recog","window","speechRecognition","webkitSpeechRecognition","onstart","console","log","onresult","event","current","resultIndex","message","results","transcript","readIttLoud","speech","SpeechSynthesisUtterance","text","volume","rate","pitch","speechSynthesis","speak","className","onClick","start","disabled","size","color","style","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"oOAAe,I,mBCuDAA,MA/Cf,WAEE,MAA4BC,IAAMC,UAAS,GAA3C,mBAAOC,EAAP,KAAeC,EAAf,KACA,EAA0BH,IAAMC,SAAS,IAAzC,mBAAOG,EAAP,KAAcC,EAAd,KAGMC,EAAQ,IADYC,OAAOC,mBAAqBD,OAAOE,yBAO7DH,EAAMI,QAAU,WACdC,QAAQC,IAAI,sBACZT,GAAU,IAGZG,EAAMO,SAAW,SAASC,GAExB,IAAMC,EAAUD,EAAME,YAChBC,EAAUH,EAAMI,QAAQH,GAAS,GAAGI,WAC1Cd,EAASY,GACTG,EAAYH,GACZd,GAAU,IAGZ,IAAMiB,EAAc,SAACH,GAEnB,IAAMI,EAAS,IAAIC,yBACnBD,EAAOE,KAAON,EACdI,EAAOG,OAAS,EAChBH,EAAOI,KAAO,EACdJ,EAAOK,MAAQ,EAEfnB,OAAOoB,gBAAgBC,MAAMP,IAI/B,OACE,sBAAKQ,UAAU,YAAf,UACE,wBAAQA,UAAU,MAAMC,QAhCd,WACZxB,EAAMyB,SA+BqCC,SAAY9B,EAArD,SAA8D,cAAC,IAAD,CAAc+B,KAAM,GAAIC,MAAM,YAC5F,qBAAIC,MAAO,CAAED,MAAM,WAAnB,cAAkChC,EAAS,aAAe,6BAA1D,OACA,6BAAKE,QCtCIgC,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.49ddbe0f.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.6ce24c58.svg\";","import logo from './logo.svg';\nimport './App.css';\nimport React from 'react';\nimport { BiMicrophone } from \"react-icons/bi\";\n\n\n\n\nfunction App() {\n\n  const [isTalk, setIsTalk] = React.useState(false)\n  const [words, setWords] = React.useState('')\n\n  const SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\n  const recog = new SpeechRecognition()\n\n  const start = () => {\n    recog.start()\n  }\n\n  recog.onstart = function(){\n    console.log(\"Voice is activated\")\n    setIsTalk(true)\n  }\n\n  recog.onresult = function(event){\n\n    const current = event.resultIndex;\n    const message = event.results[current][0].transcript\n    setWords(message)\n    readIttLoud(message)\n    setIsTalk(false)\n  }\n\n  const readIttLoud = (message) => {\n\n    const speech = new SpeechSynthesisUtterance();\n    speech.text = message;\n    speech.volume = 1;\n    speech.rate = 1;\n    speech.pitch = 1;\n\n    window.speechSynthesis.speak(speech)\n  }\n\n\n  return (\n    <div className=\"container\">\n      <button className=\"btn\" onClick={start}  disabled = {isTalk} ><BiMicrophone size={60} color=\"white\" /></button>\n      <h3 style={{ color:'#636e72' }}> {isTalk ? \"Talking...\" : \"Click button above to talk\"} </h3>\n      <h2>{words}</h2>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}